{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#print(\"greater: \", lemmatizer.lemmatize(\"greater\", pos=\"a\"))\n",
    "#print(\"greatest: \", lemmatizer.lemmatize(\"greatest\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a collection of tweets, and we are trying to predict whether or not a tweet was negative or positive. Our dataset is two text files, one labeled as all positive tweets, and another all negative tweets.\n",
    "\n",
    "First, we clean the data by removing all words that are user handles, words that contain numbers, and any words that contain non-alphabetical characters. \n",
    "\n",
    "Next, we convert our data to numerical data by using Scikit learn's CountVectorizer, which tokenzizes the all the words in the dataset. An instance of CountVectorizor after calling fit_transform contains feature names, which is a set of all the unique words in our dataset, and can be accessed by calling get_feature_names on the vectorizer. Furthermore, we can see which features each sample in our dataset contains by calling fit_transform on our dataset and then casting that to an array.\n",
    "\n",
    "Then, we split the dataset into a train and test dataset by calling train_test_split. We used 90% of the data as training data.\n",
    "\n",
    "We used a Logistic Regression to create a model for our data, and with that, acheived a 80% accuracy rate.\n",
    "\n",
    "It should be noted that the dataset was not stemmed or lemmatized. \n",
    "\n",
    "Questions:\n",
    "Since we need to provide a part of speech for each word other than nouns, how do we effectively lemmatize a dataset ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def hasOnlyAlphaChars(inputString):\n",
    "    return str.isalpha(inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the data. 1 for positive, 0 for negative.\n",
    "\n",
    "data = []\n",
    "data_labels = []\n",
    "with open(\"pos_tweets.txt\") as f:\n",
    "    for i in f:\n",
    "        data.append(i.strip())\n",
    "        data_labels.append(1)\n",
    "\n",
    "with open(\"neg_tweets.txt\") as f:\n",
    "    for i in f:\n",
    "        data.append(i.strip())\n",
    "        data_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data\n",
    "def remove_user_handles():\n",
    "    for i in range(len(data)):\n",
    "        if \"@\" in data[i]:\n",
    "            nonUserHandleWords = []\n",
    "            words = data[i].split(\" \")\n",
    "            for j in range(len(words)):\n",
    "                if \"@\" not in words[j]:\n",
    "                    nonUserHandleWords.append(words[j])\n",
    "            data[i] = \" \".join(nonUserHandleWords)\n",
    "            \n",
    "def remove_numbers():\n",
    "    for i in range(len(data)):\n",
    "        if hasNumbers(data[i]):\n",
    "            nonNumberWords = []\n",
    "            words = data[i].split(\" \")\n",
    "            for j in range(len(words)):\n",
    "                if not hasNumbers(words[j]):\n",
    "                    nonNumberWords.append(words[j])\n",
    "            data[i] = \" \".join(nonNumberWords)\n",
    "                \n",
    "def remove_non_alpha_words():\n",
    "    for i in range(len(data)):\n",
    "        if not hasOnlyAlphaChars(data[i]):\n",
    "            alphaWords = []\n",
    "            words = data[i].split(\" \")\n",
    "            for j in range(len(words)):\n",
    "                if hasOnlyAlphaChars(words[j]):\n",
    "                   alphaWords.append(words[j])\n",
    "            data[i] = \" \".join(alphaWords)\n",
    "            \n",
    "def clean_data():\n",
    "    remove_user_handles()\n",
    "    remove_numbers()\n",
    "    remove_non_alpha_words()\n",
    "\n",
    "clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization: the general process of turning a collection of text documents into numerical feature vectors\n",
    "# We can stop the use of stop words, such as \"and\" and \"the\", which are uninformative in some contexts, by specifying a stop word list \n",
    "vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "\n",
    "features = vectorizer.fit_transform(data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_nd, data_labels, train_size=0.90, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict positive or negative for our test dataset \n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posOrNeg(flag):\n",
    "    if (flag == 1):\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "    \n",
    "def getPrediction(index):\n",
    "    ind = features_nd.tolist().index(X_test[index].tolist())\n",
    "    print(\"The prediction for the tweet: '\" + data[ind] + \"' is: \" + posOrNeg(y_pred[index]) + \" . True sentiment: \" + posOrNeg(y_test[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy rate:  0.8059701492537313\n"
     ]
    }
   ],
   "source": [
    "print(\"Model accuracy rate: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for the tweet: 'miss watching Modern' is: Negative . True sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "getPrediction(84)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
